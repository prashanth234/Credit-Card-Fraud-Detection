{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.1"},"colab":{"name":"neural network-Synthetic data-Final.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"n6nBPKJX6gdg","colab_type":"code","colab":{}},"source":["import numpy as np\n","from scipy import optimize"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"M8QSa0vS6gdl","colab_type":"code","colab":{}},"source":["import pandas as pd\n","\n","def load_csv(filename):\n","    df=np.genfromtxt(filename, delimiter=',')\n","    return df\n","\n","# Find the min and max values for each column\n","def dataset_minmax(x,y):\n","    x=x/np.amax(x,axis=0)\n","    y=y/2\n","    return x,y\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5jgv4m3Z6gdr","colab_type":"code","colab":{},"outputId":"8f471d1f-a136-4208-94ca-119d59d9bb96"},"source":["e=load_csv('synthetic.csv')\n","\n","print(e.shape)\n","\n","c=e[:,0:24]\n","b=e[:,[24]]\n","print(c.shape)\n","print(b.shape)\n","\n","c,b=dataset_minmax(c,b)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(20, 25)\n","(20, 24)\n","(20, 1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cu412wjw6gdz","colab_type":"code","colab":{}},"source":["c_train=c[:15]\n","c_test=c[15:]\n","b_train=b[:15]\n","b_test=b[15:]\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"o90m32KZ6gd3","colab_type":"code","colab":{}},"source":["class trainer(object):\n","    def __init__(self, N):\n","        #Make Local reference to network:\n","        self.N = N\n","    \n","    def costFunctionWrapper(self, params, x, y):\n","        self.N.setParams(params)\n","        cost = self.N.cost(x, y)\n","        grad = self.N.computeGradients(x,y)\n","        \n","        return cost, grad\n","        \n","    def train(self, x, y):\n","        #Make an internal variable for the callback function:\n","        self.x = x\n","        self.y = y\n","        \n","        params0 = self.N.getParams()\n","\n","        options = {'maxiter': 200, 'disp' : True}\n","        _res = optimize.minimize(self.costFunctionWrapper, params0, jac=True, method='BFGS', \\\n","                                 args=(x, y), options=options)\n","\n","        self.N.setParams(_res.x)\n","        self.optimizationResults = _res"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KofXyj4z6gd8","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sjiCYx2h6gd_","colab_type":"code","colab":{}},"source":["class Neural(object):\n","   \n","    #init is a constructor and self is the instance of the object created \n","    #to sense that for which object the attributes belong to(self)\n","    def __init__(self,Lambda=0):\n","        self.noofInput=24\n","        self.noofOutput=1\n","        self.noofHidden=40\n","        self.Lambda = Lambda\n","\n","        #rando.rndm generates arandom values with mean=0 and varience=1 by guassian distribution\n","        #parameters are the size of the matrix\n","        self.w1=np.random.randn(self.noofInput,self.noofHidden)\n","        self.w2=np.random.randn(self.noofHidden,self.noofOutput)\n","        \n","        #propagate input through the network\n","    def forward(self,x):\n","        #dot function for matrix multiplication\n","        self.z2=np.dot(x,self.w1)#adding up weights to input\n","        self.a2=self.sigmoid(self.z2)#activation function to scale the value of z1\n","        self.z3=np.dot(self.a2,self.w2)\n","        yhat=self.sigmoid(self.z3)\n","        return yhat\n","    \n","   #sigmoid is a activation function \n","    def sigmoid(self,z):\n","        return 1/(1+np.exp(-z))\n","    #used in the propagation of cost function\n","    def sigmoidprime(self,z):\n","        return np.exp(-z)/(1+np.exp(-z))**2\n","    \n","    \n","    \n","    #difference to ouput and expected values i.e cost \n","    def cost(self,x,y):\n","        self.yhat=self.forward(x)\n","        #regularization\n","        #adding sum of square of our weights to cost function\n","        cost1=0.5*sum((y-self.yhat)**2)/x.shape[0] + (self.Lambda/2)*(np.sum(self.w1**2)+np.sum(self.w2**2))\n","        return cost1\n","    \n","    #using gradient descent of backpropagation computing the cost minimum\n","    #i.e derivative of j(cost) by the weights w1 and w2\n","    def costminimum(self,x,y):\n","        self.yhat=self.forward(x)\n","        delta3=np.multiply(-(y-self.yhat),self.sigmoidprime(self.z3))\n","        #regularization\n","        djw2=np.dot(self.a2.T,delta3)/x.shape[0] + self.Lambda*self.w2\n","        \n","        delta2=np.dot(delta3,self.w2.T)*self.sigmoidprime(self.z2)\n","        #regularization\n","        djw1=np.dot(x.T,delta2)/x.shape[0] + self.Lambda*self.w1\n","        \n","        return djw1,djw2\n","    \n","    #NUMERICAL CHECKINGH\n","       #Helper Functions for interacting with other classes:\n","    def getParams(self):\n","        #Get W1 and W2 unrolled into vector:\n","        params = np.concatenate((self.w1.ravel(), self.w2.ravel()))\n","        return params\n","    \n","    def setParams(self, params):\n","        #Set W1 and W2 using single paramater vector.\n","        W1_start = 0\n","        W1_end = self.noofInput * self.noofHidden\n","        self.w1 = np.reshape(params[W1_start:W1_end], (self.noofInput , self.noofHidden))\n","        W2_end = W1_end + self.noofHidden*self.noofOutput\n","        self.w2 = np.reshape(params[W1_end:W2_end], (self.noofHidden, self.noofOutput))\n","        \n","    def computeGradients(self, x, y):\n","        dJdW1, dJdW2 = self.costminimum(x, y)\n","        return np.concatenate((dJdW1.ravel(), dJdW2.ravel()))\n","    \n","    \n","    \n","\n","    \n","        "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QqRFPlKn6geE","colab_type":"code","colab":{}},"source":["def computeNumericalGradient(N, x, y):\n","        paramsInitial = N.getParams()\n","        numgrad = np.zeros(paramsInitial.shape)\n","        perturb = np.zeros(paramsInitial.shape)\n","        e = 1e-4\n","\n","        for p in range(len(paramsInitial)):\n","            #Set perturbation vector\n","            perturb[p] = e\n","            N.setParams(paramsInitial + perturb)\n","            loss2 = N.cost(x, y)\n","            \n","            N.setParams(paramsInitial - perturb)\n","            loss1 = N.cost(x, y)\n","\n","            #Compute Numerical Gradient\n","            numgrad[p] = (loss2 - loss1) / (2*e)\n","\n","            #Return the value we changed to zero:\n","            perturb[p] = 0\n","            \n","        #Return Params to original value:\n","        N.setParams(paramsInitial)\n","\n","        return numgrad"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"V_bIhFnB6geH","colab_type":"code","colab":{},"outputId":"22c38dcd-76df-4796-954b-2cda27453eab"},"source":["nn=Neural(Lambda=0.0001)\n","\n","#Make sure our gradients our correct after making changes:\n","numgrad = computeNumericalGradient(nn,c_train,b_train)\n","grad = nn.computeGradients(c_train,b_train)\n","\n","#Should be less than 1e-8:\n","#norm(grad-numgrad)/norm(grad+numgrad)\n","np.linalg.norm(grad-numgrad)/np.linalg.norm(grad+numgrad)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1.2725143825524197e-09"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"uO7FVmtU6geL","colab_type":"code","colab":{},"outputId":"4aa2bce7-ba0d-4805-f9cf-964f7b70ceb3"},"source":["nn=Neural()\n","T = trainer(nn)\n","T.train(c_train,b_train)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Optimization terminated successfully.\n","         Current function value: 0.000001\n","         Iterations: 82\n","         Function evaluations: 90\n","         Gradient evaluations: 90\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZVZMF3fe6geP","colab_type":"code","colab":{}},"source":["\n","got=nn.forward(c_test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_jkF8_q36geS","colab_type":"code","colab":{},"outputId":"b0dfac04-927f-4db2-fc69-67e9153f13f3"},"source":["print(got)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[ 0.74558867]\n"," [ 0.98823659]\n"," [ 0.9089155 ]\n"," [ 0.33760053]\n"," [ 0.99768858]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TGCw-zZN6geV","colab_type":"code","colab":{}},"source":["i=0\n","bad=0\n","good=0\n","unmatched=0\n","for i in range(0,5):\n","    if(b_test[i]==1):\n","        if(got[i]>0.6):\n","            good=good+1\n","        else:\n","            unmatched=unmatched+1\n","    else:\n","        if(got[i]<=0.6):\n","            bad=bad+1\n","        else:\n","            unmatched=unmatched+1\n","        \n","        "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fCO9Ydc46geY","colab_type":"code","colab":{},"outputId":"722235eb-2e2d-4b81-fa18-63e95fb7c4ae"},"source":["print(good,bad,unmatched)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["2 1 2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZtmXG_5j6ged","colab_type":"code","colab":{}},"source":["i=0\n","bad_o=0\n","good_o=0\n","for i in range(0,5):\n","    if(b_test[i]!=0.5):\n","        good_o=good_o+1\n","    else:\n","        bad_o=bad_o+1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"H2CE2WvS6geg","colab_type":"code","colab":{},"outputId":"493a92d2-d3f4-4ea3-b974-9b62773358e6"},"source":["print(good_o,bad_o)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["2 3\n"],"name":"stdout"}]}]}