{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_csv(filename):\n",
    "    df=np.genfromtxt(filename, delimiter=',')\n",
    "    return df\n",
    "\n",
    "# Find the min and max values for each column\n",
    "def dataset_minmax(x,y):\n",
    "    x=x/np.amax(x,axis=0)\n",
    "    y=y/2\n",
    "    return x,y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 25)\n",
      "(1000, 24)\n",
      "(1000, 1)\n"
     ]
    }
   ],
   "source": [
    "e=load_csv('cdnumeric.csv')\n",
    "\n",
    "print(e.shape)\n",
    "\n",
    "c=e[:,0:24]\n",
    "b=e[:,[24]]\n",
    "print(c.shape)\n",
    "print(b.shape)\n",
    "\n",
    "c,b=dataset_minmax(c,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c_train=c[:750]\n",
    "c_test=c[750:]\n",
    "b_train=b[:750]\n",
    "b_test=b[750:]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class trainer(object):\n",
    "    def __init__(self, N):\n",
    "        #Make Local reference to network:\n",
    "        self.N = N\n",
    "    \n",
    "    def costFunctionWrapper(self, params, x, y):\n",
    "        self.N.setParams(params)\n",
    "        cost = self.N.cost(x, y)\n",
    "        grad = self.N.computeGradients(x,y)\n",
    "        \n",
    "        return cost, grad\n",
    "        \n",
    "    def train(self, x, y):\n",
    "        #Make an internal variable for the callback function:\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "        params0 = self.N.getParams()\n",
    "\n",
    "        options = {'maxiter': 200, 'disp' : True}\n",
    "        _res = optimize.minimize(self.costFunctionWrapper, params0, jac=True, method='BFGS', \\\n",
    "                                 args=(x, y), options=options)\n",
    "\n",
    "        self.N.setParams(_res.x)\n",
    "        self.optimizationResults = _res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Neural(object):\n",
    "   \n",
    "    #init is a constructor and self is the instance of the object created \n",
    "    #to sense that for which object the attributes belong to(self)\n",
    "    def __init__(self,Lambda=0):\n",
    "        self.noofInput=24\n",
    "        self.noofOutput=1\n",
    "        self.noofHidden=40\n",
    "        self.Lambda = Lambda\n",
    "\n",
    "        #rando.rndm generates arandom values with mean=0 and varience=1 by guassian distribution\n",
    "        #parameters are the size of the matrix\n",
    "        self.w1=np.random.randn(self.noofInput,self.noofHidden)\n",
    "        self.w2=np.random.randn(self.noofHidden,self.noofOutput)\n",
    "        \n",
    "        #propagate input through the network\n",
    "    def forward(self,x):\n",
    "        #dot function for matrix multiplication\n",
    "        self.z2=np.dot(x,self.w1)#adding up weights to input\n",
    "        self.a2=self.sigmoid(self.z2)#activation function to scale the value of z1\n",
    "        self.z3=np.dot(self.a2,self.w2)\n",
    "        yhat=self.sigmoid(self.z3)\n",
    "        return yhat\n",
    "    \n",
    "   #sigmoid is a activation function \n",
    "    def sigmoid(self,z):\n",
    "        return 1/(1+np.exp(-z))\n",
    "    #used in the propagation of cost function\n",
    "    def sigmoidprime(self,z):\n",
    "        return np.exp(-z)/(1+np.exp(-z))**2\n",
    "    \n",
    "    \n",
    "    \n",
    "    #difference to ouput and expected values i.e cost \n",
    "    def cost(self,x,y):\n",
    "        self.yhat=self.forward(x)\n",
    "        #regularization\n",
    "        #adding sum of square of our weights to cost function\n",
    "        cost1=0.5*sum((y-self.yhat)**2)/x.shape[0] + (self.Lambda/2)*(np.sum(self.w1**2)+np.sum(self.w2**2))\n",
    "        return cost1\n",
    "    \n",
    "    #using gradient descent of backpropagation computing the cost minimum\n",
    "    #i.e derivative of j(cost) by the weights w1 and w2\n",
    "    def costminimum(self,x,y):\n",
    "        self.yhat=self.forward(x)\n",
    "        delta3=np.multiply(-(y-self.yhat),self.sigmoidprime(self.z3))\n",
    "        #regularization\n",
    "        djw2=np.dot(self.a2.T,delta3)/x.shape[0] + self.Lambda*self.w2\n",
    "        \n",
    "        delta2=np.dot(delta3,self.w2.T)*self.sigmoidprime(self.z2)\n",
    "        #regularization\n",
    "        djw1=np.dot(x.T,delta2)/x.shape[0] + self.Lambda*self.w1\n",
    "        \n",
    "        return djw1,djw2\n",
    "    \n",
    "    #NUMERICAL CHECKINGH\n",
    "       #Helper Functions for interacting with other classes:\n",
    "    def getParams(self):\n",
    "        #Get W1 and W2 unrolled into vector:\n",
    "        params = np.concatenate((self.w1.ravel(), self.w2.ravel()))\n",
    "        return params\n",
    "    \n",
    "    def setParams(self, params):\n",
    "        #Set W1 and W2 using single paramater vector.\n",
    "        W1_start = 0\n",
    "        W1_end = self.noofInput * self.noofHidden\n",
    "        self.w1 = np.reshape(params[W1_start:W1_end], (self.noofInput , self.noofHidden))\n",
    "        W2_end = W1_end + self.noofHidden*self.noofOutput\n",
    "        self.w2 = np.reshape(params[W1_end:W2_end], (self.noofHidden, self.noofOutput))\n",
    "        \n",
    "    def computeGradients(self, x, y):\n",
    "        dJdW1, dJdW2 = self.costminimum(x, y)\n",
    "        return np.concatenate((dJdW1.ravel(), dJdW2.ravel()))\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeNumericalGradient(N, x, y):\n",
    "        paramsInitial = N.getParams()\n",
    "        numgrad = np.zeros(paramsInitial.shape)\n",
    "        perturb = np.zeros(paramsInitial.shape)\n",
    "        e = 1e-4\n",
    "\n",
    "        for p in range(len(paramsInitial)):\n",
    "            #Set perturbation vector\n",
    "            perturb[p] = e\n",
    "            N.setParams(paramsInitial + perturb)\n",
    "            loss2 = N.cost(x, y)\n",
    "            \n",
    "            N.setParams(paramsInitial - perturb)\n",
    "            loss1 = N.cost(x, y)\n",
    "\n",
    "            #Compute Numerical Gradient\n",
    "            numgrad[p] = (loss2 - loss1) / (2*e)\n",
    "\n",
    "            #Return the value we changed to zero:\n",
    "            perturb[p] = 0\n",
    "            \n",
    "        #Return Params to original value:\n",
    "        N.setParams(paramsInitial)\n",
    "\n",
    "        return numgrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2002307337347683e-10"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn=Neural(Lambda=0.0001)\n",
    "\n",
    "#Make sure our gradients our correct after making changes:\n",
    "numgrad = computeNumericalGradient(nn,c_train,b_train)\n",
    "grad = nn.computeGradients(c_train,b_train)\n",
    "\n",
    "#Should be less than 1e-8:\n",
    "#norm(grad-numgrad)/norm(grad+numgrad)\n",
    "np.linalg.norm(grad-numgrad)/np.linalg.norm(grad+numgrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.015118\n",
      "         Iterations: 200\n",
      "         Function evaluations: 214\n",
      "         Gradient evaluations: 214\n"
     ]
    }
   ],
   "source": [
    "nn=Neural()\n",
    "T = trainer(nn)\n",
    "T.train(c_train,b_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "got=nn.forward(c_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(got)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i=0\n",
    "bad=0\n",
    "good=0\n",
    "unmatched=0\n",
    "for i in range(0,250):\n",
    "    if(b_test[i]==1):\n",
    "        if(got[i]>0.6):\n",
    "            good=good+1\n",
    "        else:\n",
    "            unmatched=unmatched+1\n",
    "    else:\n",
    "        if(got[i]<=0.6):\n",
    "            bad=bad+1\n",
    "        else:\n",
    "            unmatched=unmatched+1\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56 107 87\n"
     ]
    }
   ],
   "source": [
    "print(good,bad,unmatched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i=0\n",
    "bad_o=0\n",
    "good_o=0\n",
    "for i in range(0,250):\n",
    "    if(b_test[i]!=0.5):\n",
    "        good_o=good_o+1\n",
    "    else:\n",
    "        bad_o=bad_o+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77 173\n"
     ]
    }
   ],
   "source": [
    "print(good_o,bad_o)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
